{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activity</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>...</th>\n",
       "      <th>D1767</th>\n",
       "      <th>D1768</th>\n",
       "      <th>D1769</th>\n",
       "      <th>D1770</th>\n",
       "      <th>D1771</th>\n",
       "      <th>D1772</th>\n",
       "      <th>D1773</th>\n",
       "      <th>D1774</th>\n",
       "      <th>D1775</th>\n",
       "      <th>D1776</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "      <td>3751.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.542255</td>\n",
       "      <td>0.076948</td>\n",
       "      <td>0.592436</td>\n",
       "      <td>0.068142</td>\n",
       "      <td>0.038990</td>\n",
       "      <td>0.212112</td>\n",
       "      <td>0.686653</td>\n",
       "      <td>0.274713</td>\n",
       "      <td>0.455133</td>\n",
       "      <td>0.749517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026926</td>\n",
       "      <td>0.014663</td>\n",
       "      <td>0.013863</td>\n",
       "      <td>0.021861</td>\n",
       "      <td>0.015196</td>\n",
       "      <td>0.016796</td>\n",
       "      <td>0.012263</td>\n",
       "      <td>0.011730</td>\n",
       "      <td>0.020261</td>\n",
       "      <td>0.011197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.498278</td>\n",
       "      <td>0.079989</td>\n",
       "      <td>0.105860</td>\n",
       "      <td>0.078414</td>\n",
       "      <td>0.115885</td>\n",
       "      <td>0.102592</td>\n",
       "      <td>0.078702</td>\n",
       "      <td>0.090017</td>\n",
       "      <td>0.162731</td>\n",
       "      <td>0.071702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161889</td>\n",
       "      <td>0.120215</td>\n",
       "      <td>0.116938</td>\n",
       "      <td>0.146249</td>\n",
       "      <td>0.122348</td>\n",
       "      <td>0.128522</td>\n",
       "      <td>0.110074</td>\n",
       "      <td>0.107683</td>\n",
       "      <td>0.140911</td>\n",
       "      <td>0.105236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>0.137873</td>\n",
       "      <td>0.006130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.275590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.517811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138118</td>\n",
       "      <td>0.625627</td>\n",
       "      <td>0.207374</td>\n",
       "      <td>0.378062</td>\n",
       "      <td>0.707339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.585989</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190926</td>\n",
       "      <td>0.674037</td>\n",
       "      <td>0.277845</td>\n",
       "      <td>0.499942</td>\n",
       "      <td>0.738961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.668395</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261726</td>\n",
       "      <td>0.740663</td>\n",
       "      <td>0.335816</td>\n",
       "      <td>0.569962</td>\n",
       "      <td>0.788177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964381</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994735</td>\n",
       "      <td>0.790831</td>\n",
       "      <td>0.989870</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 1777 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Activity           D1           D2           D3           D4  \\\n",
       "count  3751.000000  3751.000000  3751.000000  3751.000000  3751.000000   \n",
       "mean      0.542255     0.076948     0.592436     0.068142     0.038990   \n",
       "std       0.498278     0.079989     0.105860     0.078414     0.115885   \n",
       "min       0.000000     0.000000     0.282128     0.000000     0.000000   \n",
       "25%       0.000000     0.033300     0.517811     0.000000     0.000000   \n",
       "50%       1.000000     0.066700     0.585989     0.050000     0.000000   \n",
       "75%       1.000000     0.100000     0.668395     0.100000     0.000000   \n",
       "max       1.000000     1.000000     0.964381     0.950000     1.000000   \n",
       "\n",
       "                D5           D6           D7           D8           D9  ...  \\\n",
       "count  3751.000000  3751.000000  3751.000000  3751.000000  3751.000000  ...   \n",
       "mean      0.212112     0.686653     0.274713     0.455133     0.749517  ...   \n",
       "std       0.102592     0.078702     0.090017     0.162731     0.071702  ...   \n",
       "min       0.002630     0.137873     0.006130     0.000000     0.275590  ...   \n",
       "25%       0.138118     0.625627     0.207374     0.378062     0.707339  ...   \n",
       "50%       0.190926     0.674037     0.277845     0.499942     0.738961  ...   \n",
       "75%       0.261726     0.740663     0.335816     0.569962     0.788177  ...   \n",
       "max       1.000000     0.994735     0.790831     0.989870     1.000000  ...   \n",
       "\n",
       "             D1767        D1768        D1769        D1770        D1771  \\\n",
       "count  3751.000000  3751.000000  3751.000000  3751.000000  3751.000000   \n",
       "mean      0.026926     0.014663     0.013863     0.021861     0.015196   \n",
       "std       0.161889     0.120215     0.116938     0.146249     0.122348   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             D1772        D1773        D1774        D1775        D1776  \n",
       "count  3751.000000  3751.000000  3751.000000  3751.000000  3751.000000  \n",
       "mean      0.016796     0.012263     0.011730     0.020261     0.011197  \n",
       "std       0.128522     0.110074     0.107683     0.140911     0.105236  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  \n",
       "\n",
       "[8 rows x 1777 columns]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('./bioresponse.csv')\n",
    "df_x = df.drop('Activity', axis=1).to_numpy()\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(df_x, df.Activity.to_numpy(), test_size = 0.25)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCHASTIC_AMOUNT = 0.2\n",
    "M_DECAY = 0.9\n",
    "V_DECAY = 0.9999\n",
    "EPSILON = 0.000000001\n",
    "\n",
    "def sigmoid(z):\n",
    "    s = 1./(1.+np.exp(-z))\n",
    "    return s\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0.\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    m = X.shape[1]\n",
    "    #print('number of objects = ',len(X))\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    A = sigmoid(np.dot(w.T,X)+b )                            # compute activation\n",
    "    cost = -(1./m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A),axis=1)   # compute cost\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    dw = (1./m)*np.dot(X,(A-Y).T)\n",
    "    db = (1./m)*np.sum(A-Y,axis=1)\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    "def optimize_sgd(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "                \n",
    "        # Cost and gradient calculation\n",
    "        #take only a small sample and calculate within it\n",
    "        stochastic_data_indexes = random.sample(range(0, X.shape[1]), int(STOCHASTIC_AMOUNT * X.shape[1]))\n",
    "\n",
    "        grads, cost = propagate(\n",
    "            w,\n",
    "            b, \n",
    "            X[:,stochastic_data_indexes], \n",
    "            Y[stochastic_data_indexes]\n",
    "        )\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule\n",
    "        w -=learning_rate*dw\n",
    "        b -=learning_rate*db\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "                \n",
    "        # Cost and gradient calculation \n",
    "        grads, cost = propagate(w,b,X,Y)\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule\n",
    "        w -=learning_rate*dw\n",
    "        b -=learning_rate*db\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "\n",
    "def predict(w, b, X):\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities \n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        if (A[0,i]<=0.5):\n",
    "            Y_prediction[0][i]=0\n",
    "        else:\n",
    "            Y_prediction[0][i]=1\n",
    "    \n",
    "    return Y_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_adam(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    costs = []\n",
    "\n",
    "    m_w = np.zeros_like(w)\n",
    "    v_w = np.zeros_like(w)\n",
    "\n",
    "    m_b = 0.\n",
    "    v_b = 0.\n",
    "\n",
    "    for i in range(1, num_iterations + 1):\n",
    "                \n",
    "        # Cost and gradient calculation \n",
    "        grads, cost = propagate(w,b,X,Y)\n",
    "        \n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "\n",
    "        m_w = M_DECAY * m_w + (1-M_DECAY) * dw\n",
    "        v_w = V_DECAY * v_w + (1-V_DECAY) * dw ** 2\n",
    "\n",
    "        m_b = M_DECAY * m_b + (1-M_DECAY) * db\n",
    "        v_b = V_DECAY * v_b + (1-V_DECAY) * db ** 2\n",
    "\n",
    "        m_w_corr = m_w / (1-M_DECAY ** i)\n",
    "        v_w_corr = v_w / (1-V_DECAY ** i)\n",
    "\n",
    "        m_b_corr = m_b / (1-M_DECAY ** i)\n",
    "        v_b_corr = v_b / (1-V_DECAY ** i)\n",
    "\n",
    "        w -= learning_rate * m_w_corr / (np.sqrt(v_w_corr) + EPSILON)\n",
    "        b -= learning_rate * m_b_corr / (np.sqrt(v_b_corr) + EPSILON)\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False, optimization_function = 'gd'):\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent\n",
    "    if (optimization_function == 'sgd'):\n",
    "        parameters, grads, costs = optimize_sgd(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    elif (optimization_function == 'adam'):\n",
    "        parameters, grads, costs = optimize_adam(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    else:\n",
    "        parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "\n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 1000: 0.090389\n",
      "Cost after iteration 2000: 0.080190\n",
      "Cost after iteration 3000: 0.061436\n",
      "Cost after iteration 4000: nan\n",
      "Cost after iteration 5000: nan\n",
      "Cost after iteration 6000: nan\n",
      "Cost after iteration 7000: nan\n",
      "Cost after iteration 8000: nan\n",
      "Cost after iteration 9000: nan\n",
      "Cost after iteration 10000: nan\n",
      "train accuracy: 96.40952719516531 %\n",
      "test accuracy: 66.09808102345416 %\n"
     ]
    }
   ],
   "source": [
    "M_DECAY = 0.9\n",
    "V_DECAY = 0.9\n",
    "d = model(train_data.T, train_labels.T, test_data.T, test_labels.T, num_iterations = 10000, learning_rate = 0.3, print_cost = True, optimization_function='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 1000: 0.341449\n",
      "Cost after iteration 2000: 0.333438\n",
      "Cost after iteration 3000: 0.293979\n",
      "Cost after iteration 4000: 0.324166\n",
      "Cost after iteration 5000: 0.270456\n",
      "Cost after iteration 6000: 0.300972\n",
      "Cost after iteration 7000: 0.309771\n",
      "Cost after iteration 8000: 0.257565\n",
      "Cost after iteration 9000: 0.278893\n",
      "train accuracy: 89.51297547102737 %\n",
      "test accuracy: 73.88059701492537 %\n"
     ]
    }
   ],
   "source": [
    "d = model(train_data.T, train_labels.T, test_data.T, test_labels.T, num_iterations = 10000, learning_rate = 0.3, print_cost = True, optimization_function='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 1000: 0.379715\n",
      "Cost after iteration 2000: 0.325111\n",
      "Cost after iteration 3000: 0.310812\n",
      "Cost after iteration 4000: 0.300541\n",
      "Cost after iteration 5000: 0.292511\n",
      "Cost after iteration 6000: 0.285907\n",
      "Cost after iteration 7000: 0.280291\n",
      "Cost after iteration 8000: 0.275401\n",
      "Cost after iteration 9000: 0.271069\n",
      "train accuracy: 89.37077852826164 %\n",
      "test accuracy: 74.30703624733476 %\n"
     ]
    }
   ],
   "source": [
    "d = model(train_data.T, train_labels.T, test_data.T, test_labels.T, num_iterations = 10000, learning_rate = 0.3, print_cost = True, optimization_function='gd')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5c9fbd331fea3c481f4404cc8af350459f007b4c4df3c5795a543f0770733c9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
