{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/patrik-\n",
      "[nltk_data]     sh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/patrik-sh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/patrik-\n",
      "[nltk_data]     sh/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/patrik-\n",
      "[nltk_data]     sh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text downloaded'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "alice_txt = open('alice.txt', encoding='utf8').read()\n",
    "\n",
    "'text downloaded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             alice\n",
       "3         adventure\n",
       "5        wonderland\n",
       "7             lewis\n",
       "8           carroll\n",
       "            ...    \n",
       "34622    child-life\n",
       "34626         happy\n",
       "34627        summer\n",
       "34628           day\n",
       "34631           end\n",
       "Length: 12507, dtype: object"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text preprocessing\n",
    "\n",
    "#remove all new lines, lowercase\n",
    "alice_txt = alice_txt.replace(r'\\n', ' ')\n",
    "alice_txt = alice_txt.lower()\n",
    "\n",
    "#delete the useless beginning and the ending \n",
    "#BEGINNING = '[Illustration]'\n",
    "#ENDING = 'THE END'\n",
    "#print(len(alice_txt.split(BEGINNING)))\n",
    "#alice_txt = alice_txt.split(BEGINNING)[1]\n",
    "\n",
    "alice_tokenized = pd.Series(word_tokenize(alice_txt))\n",
    "\n",
    "alice_no_stopwords = alice_tokenized.loc[~alice_tokenized.isin(stopwords.words('english'))]\n",
    "\n",
    "alice_no_punction = alice_no_stopwords.loc[~alice_no_stopwords.isin(list(string.punctuation + string.digits + \"'\" + '\"' + '”' + '“' + '’'))]\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "alice_preprocessed = alice_no_punction.apply(lambda word: lemmatizer.lemmatize(word))\n",
    "\n",
    "alice_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----for chapter 1----\n",
      "       word  count\n",
      "225  little     15\n",
      "93     like     11\n",
      "50      way     11\n",
      "84      see     10\n",
      "48    think      9\n",
      "296    door      9\n",
      "126     one      8\n",
      "61     time      8\n",
      "22    could      8\n",
      "159    said      8\n",
      "----for chapter 2----\n",
      "       word  count\n",
      "367   mouse     16\n",
      "29   little     16\n",
      "107    said     12\n",
      "34     dear     11\n",
      "52       go     11\n",
      "20     foot     10\n",
      "171   thing     10\n",
      "15     like      9\n",
      "2      pool      9\n",
      "42     must      9\n",
      "----for chapter 3----\n",
      "      word  count\n",
      "55    said     34\n",
      "56   mouse     21\n",
      "170   dodo     12\n",
      "45    know     11\n",
      "90       ‘      9\n",
      "202    one      8\n",
      "3     long      7\n",
      "65    soon      7\n",
      "10    bird      6\n",
      "41   would      6\n",
      "----for chapter 4----\n",
      "        word  count\n",
      "4     little     24\n",
      "2     rabbit     16\n",
      "199      one     14\n",
      "77      said     14\n",
      "5       bill     13\n",
      "225  thought      9\n",
      "25      sure      9\n",
      "15     heard      9\n",
      "23       get      9\n",
      "113    thing      8\n",
      "----for chapter 5----\n",
      "            word  count\n",
      "16          said     52\n",
      "3    caterpillar     27\n",
      "382       pigeon     12\n",
      "367      serpent     12\n",
      "55          well     10\n",
      "64        little     10\n",
      "119       minute      8\n",
      "32         think      7\n",
      "52          size      7\n",
      "155        youth      6\n",
      "----for chapter 6----\n",
      "        word  count\n",
      "96      said     47\n",
      "225      cat     24\n",
      "34      like     16\n",
      "45    little     14\n",
      "60   duchess     14\n",
      "12   footman     12\n",
      "77      much     12\n",
      "209     baby     11\n",
      "21     would     11\n",
      "2        pig     10\n",
      "----for chapter 7----\n",
      "         word  count\n",
      "39       said     58\n",
      "11     hatter     33\n",
      "13   dormouse     27\n",
      "9       march     21\n",
      "10       hare     21\n",
      "68       time     17\n",
      "109     thing     12\n",
      "113      well     10\n",
      "29        one     10\n",
      "103      went     10\n",
      "----for chapter 8----\n",
      "         word  count\n",
      "35       said     42\n",
      "2       queen     38\n",
      "100      head     16\n",
      "144      king     14\n",
      "394       cat     11\n",
      "13      three     11\n",
      "315  hedgehog     10\n",
      "26        one     10\n",
      "21       went     10\n",
      "24       came      9\n",
      "----for chapter 9----\n",
      "        word  count\n",
      "11      said     57\n",
      "2       mock     27\n",
      "3     turtle     27\n",
      "313  gryphon     20\n",
      "12   duchess     19\n",
      "238    queen     14\n",
      "42      went     13\n",
      "191    never     11\n",
      "117        ‘     10\n",
      "64    little     10\n",
      "----for chapter 10----\n",
      "          word  count\n",
      "24        said     45\n",
      "25     gryphon     31\n",
      "5       turtle     30\n",
      "4         mock     28\n",
      "118      would     15\n",
      "2      lobster     14\n",
      "57       dance     13\n",
      "432       soup     11\n",
      "431  beautiful     11\n",
      "21       voice     10\n",
      "----for chapter 11----\n",
      "         word  count\n",
      "74       said     38\n",
      "4        king     26\n",
      "190    hatter     20\n",
      "37      court     15\n",
      "209  dormouse     13\n",
      "32        one     11\n",
      "5       queen      9\n",
      "188   witness      8\n",
      "53    thought      8\n",
      "30     rabbit      8\n",
      "----for chapter 12----\n",
      "       word  count\n",
      "56     said     50\n",
      "57     king     22\n",
      "52    would     12\n",
      "172   queen     10\n",
      "72   little     10\n",
      "89     jury      9\n",
      "20     head      8\n",
      "87      one      8\n",
      "120   white      8\n",
      "121  rabbit      8\n"
     ]
    }
   ],
   "source": [
    "#divide by chapters creating an dataframe structure [chapter,word]\n",
    "\n",
    "chapterized_alice_arr = []\n",
    "\n",
    "chapter_nmb = 0\n",
    "for word in alice_preprocessed.iteritems():\n",
    "  if (word[1] == 'chapter'):\n",
    "    if chapter_nmb == 12:\n",
    "      chapter_nmb = 1\n",
    "    else:\n",
    "      chapter_nmb += 1\n",
    "  chapterized_alice_arr.append({ 'chapter': chapter_nmb, 'word': word[1] })\n",
    "\n",
    "chapterized_alice = pd.DataFrame(chapterized_alice_arr)\n",
    "\n",
    "def get_10_frequent_words(df: pd.DataFrame, chapter: int):\n",
    "  df_working = df[df.chapter == chapter]\n",
    "  unique_words = df_working[df_working.word != 'alice'].word.unique()\n",
    "\n",
    "  words_count = []\n",
    "\n",
    "  for w in unique_words:\n",
    "    words_count.append({\n",
    "      'word': w,\n",
    "      'count': df_working.loc[df_working.word == w].word.count()\n",
    "    })\n",
    "  \n",
    "  counted = pd.DataFrame(words_count)\n",
    "  return counted.sort_values(by=['count'], ascending=False).head(10)\n",
    "\n",
    "for chap in range(1,13):\n",
    "  print(f'----for chapter {chap}----')\n",
    "  print(get_10_frequent_words(chapterized_alice, chap))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5c9fbd331fea3c481f4404cc8af350459f007b4c4df3c5795a543f0770733c9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
